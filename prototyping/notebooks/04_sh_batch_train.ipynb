{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_sh_batch_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19ofG2-rDUrzkkV4ITuT4NHu3iVNAIFtb",
      "authorship_tag": "ABX9TyNwUgLSPFVhNEpKGCK3OnQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharsulkar/H1B_LCA_outcome_prediction/blob/main/prototyping/notebooks/04_sh_batch_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYsW3ousCq1F"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(42)\r\n",
        "import pandas as pd\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\r\n",
        "from sklearn.compose import make_column_transformer\r\n",
        "from pickle import dump, load\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import f1_score, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNM78_2gCzSq"
      },
      "source": [
        "def read_csv_to_list(filepath,header=None,squeeze=True):\r\n",
        "  return list(pd.read_csv(filepath,header=None,squeeze=True))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WENahbypDC1c"
      },
      "source": [
        "#Custom transformer to drop rows based on filter\r\n",
        "class droprows_Transformer(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self):\r\n",
        "      self.row_index = None # row index to drop\r\n",
        "      self.inplace=True\r\n",
        "      self.reset_index=True\r\n",
        "\r\n",
        "    def fit( self, X, y=None):\r\n",
        "      return self \r\n",
        "    \r\n",
        "    def transform(self, X, y=None):\r\n",
        "      self.row_index=X[~X.CASE_STATUS.isin(['Certified','Denied'])].index\r\n",
        "      X.drop(index=self.row_index,inplace=self.inplace)\r\n",
        "      if self.reset_index:\r\n",
        "        X.reset_index(inplace=True,drop=True)\r\n",
        "      return X"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyobp2xWDDih"
      },
      "source": [
        "class buildfeatures_Transformer(BaseEstimator, TransformerMixin):\r\n",
        "  def __init__(self, input_columns):\r\n",
        "    self.input_columns=input_columns\r\n",
        "\r\n",
        "  def date_diff(self,date1,date2):\r\n",
        "    return date1-date2\r\n",
        "\r\n",
        "  def is_USA(self,country):\r\n",
        "    if country=='UNITED STATES OF AMERICA':\r\n",
        "      USA_YN='Y' \r\n",
        "    else:\r\n",
        "      USA_YN='N'\r\n",
        "    return USA_YN\r\n",
        "\r\n",
        "  def fit(self, X, y=None):\r\n",
        "    return self\r\n",
        "\r\n",
        "  def transform(self, X, y=None):\r\n",
        "    # Processing_Days and Validity_days\r\n",
        "    X['PROCESSING_DAYS']=self.date_diff(X.DECISION_DATE, X.RECEIVED_DATE).dt.days\r\n",
        "    X['VALIDITY_DAYS']=self.date_diff(X.END_DATE, X.BEGIN_DATE).dt.days\r\n",
        "\r\n",
        "    # SOC_Codes\r\n",
        "    X['SOC_CD2']=X.SOC_CODE.str.split(pat='-',n=1,expand=True)[0]\r\n",
        "    X['SOC_CD4']=X.SOC_CODE.str.split(pat='-',n=1,expand=True)[1].str.split(pat='.',n=1,expand=True)[0]\r\n",
        "    X['SOC_CD_ONET']=X.SOC_CODE.str.split(pat='-',n=1,expand=True)[1].str.split(pat='.',n=1,expand=True)[1]\r\n",
        "\r\n",
        "    # USA_YN\r\n",
        "    X['USA_YN']=X.EMPLOYER_COUNTRY.apply(self.is_USA)\r\n",
        "\r\n",
        "    # Employer_Worksite_YN\r\n",
        "    X['EMPLOYER_WORKSITE_YN']='Y'\r\n",
        "    X.loc[X.EMPLOYER_POSTAL_CODE.ne(X.WORKSITE_POSTAL_CODE),'EMPLOYER_WORKSITE_YN']='N'\r\n",
        "\r\n",
        "    # OES_YN\r\n",
        "    X['OES_YN']='Y'\r\n",
        "    X.iloc[X[~X.PW_OTHER_SOURCE.isna()].index,X.columns.get_loc('OES_YN')]='N'\r\n",
        "\r\n",
        "    # SURVEY_YEAR\r\n",
        "    X['SURVEY_YEAR']=pd.to_datetime(X.PW_OES_YEAR.str.split(pat='-',n=1,expand=True)[0]).dt.to_period('Y')\r\n",
        "    PW_other_year=X[X.OES_YN=='N'].PW_OTHER_YEAR\r\n",
        "    #Rename the series and update dataframe with series object\r\n",
        "    PW_other_year.rename(\"SURVEY_YEAR\",inplace=True)\r\n",
        "    X.update(PW_other_year)\r\n",
        "\r\n",
        "    # WAGE_ABOVE_PREVAILING_HR\r\n",
        "    X['WAGE_PER_HR']=X.WAGE_RATE_OF_PAY_FROM\r\n",
        "    #compute for Year\r\n",
        "    X.iloc[X[X.WAGE_UNIT_OF_PAY=='Year'].index,X.columns.get_loc('WAGE_PER_HR')]=X[X.WAGE_UNIT_OF_PAY=='Year'].WAGE_RATE_OF_PAY_FROM/2067\r\n",
        "    #compute for Month\r\n",
        "    X.iloc[X[X.WAGE_UNIT_OF_PAY=='Month'].index,X.columns.get_loc('WAGE_PER_HR')]=X[X.WAGE_UNIT_OF_PAY=='Month'].WAGE_RATE_OF_PAY_FROM/172\r\n",
        "\r\n",
        "    #initialize with WAGE_RATE_OF_PAY_FROM\r\n",
        "    X['PW_WAGE_PER_HR']=X.PREVAILING_WAGE\r\n",
        "    #compute for Year\r\n",
        "    X.iloc[X[X.PW_UNIT_OF_PAY=='Year'].index,X.columns.get_loc('PW_WAGE_PER_HR')]=X[X.PW_UNIT_OF_PAY=='Year'].PREVAILING_WAGE/2067\r\n",
        "    #compute for Month\r\n",
        "    X.iloc[X[X.PW_UNIT_OF_PAY=='Month'].index,X.columns.get_loc('PW_WAGE_PER_HR')]=X[X.PW_UNIT_OF_PAY=='Month'].PREVAILING_WAGE/172\r\n",
        "\r\n",
        "    X['WAGE_ABOVE_PW_HR']=X.WAGE_PER_HR-X.PW_WAGE_PER_HR\r\n",
        "\r\n",
        "    return X"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5dMKmJJDGE6"
      },
      "source": [
        "#Custom transformer to drop features for input feature list\r\n",
        "class dropfeatures_Transformer(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, columns, inplace):\r\n",
        "      self.columns = columns # list of categorical columns in input Dataframe\r\n",
        "      self.inplace=True\r\n",
        "\r\n",
        "    def fit( self, X, y=None):\r\n",
        "      return self \r\n",
        "    \r\n",
        "    def transform(self, X, y=None):\r\n",
        "      X.drop(columns=self.columns,inplace=self.inplace)\r\n",
        "      return X"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RovSTIjQDH05"
      },
      "source": [
        "#Custom transformer to compute Random Standard encoding for categorical features for incrementaly encoding data\r\n",
        "class RSE_Transformer(BaseEstimator, TransformerMixin):\r\n",
        "    #Class Constructor\r\n",
        "    def __init__( self, cat_cols, categories=None, RSE=None ):\r\n",
        "        self.cat_cols = cat_cols # list of categorical columns in input Dataframe\r\n",
        "        self.categories = None # Array of unique non-numeric values in each categorical column\r\n",
        "        self.RSE = None # Array of Random Standard encoding for each row in categories\r\n",
        "        \r\n",
        "    def fit( self, X, y=None ):\r\n",
        "      #Get a list of all unique categorical values for each column\r\n",
        "      if self.categories is None:\r\n",
        "        self.categories = [X[column].unique() for column in cat_cols]\r\n",
        "\r\n",
        "        #replace missing values and append missing value label to each column to handle missing values in test dataset that might not be empty in train dataset\r\n",
        "        for i in range(len(self.categories)):\r\n",
        "          if np.array(self.categories[i].astype(str)!=str(np.nan)).all():\r\n",
        "            self.categories[i]=np.append(self.categories[i],np.nan)\r\n",
        "\r\n",
        "        #compute RandomStandardEncoding \r\n",
        "        self.RSE=[np.random.normal(0,1,len(self.categories[i])) for i in range(len(self.cat_cols))]\r\n",
        "\r\n",
        "      else:\r\n",
        "        for i in range(len(self.cat_cols)):\r\n",
        "          #append new unique categories to self.categories\r\n",
        "          new_categories=list(set(X[self.cat_cols[i]].unique()).difference(set(self.categories[i])))\r\n",
        "          if new_categories!=[]:\r\n",
        "            #print('not empty') #replace with logging call\r\n",
        "            #print('categories before append',len(categories[i])) #logging call\r\n",
        "            self.categories[i]=np.append(self.categories[i],new_categories) #append new categories to the end\r\n",
        "            new_RSE=np.random.normal(0,1,len(new_categories)) #generate new RSE values\r\n",
        "            #regenrate if overlap found with existing encodings\r\n",
        "            if set(new_RSE).issubset(set(self.RSE[i])): \r\n",
        "              #print('yes') #loggin call\r\n",
        "              new_RSE=np.random.normal(0,1,len(new_categories))\r\n",
        "            \r\n",
        "            self.RSE[i]=np.append(self.RSE[i],new_RSE) #append new RSE values\r\n",
        "          #print('new categories',len(new_categories)) #logging call\r\n",
        "          #print('categories after append',len(categories[i]))\r\n",
        "     \r\n",
        "      return self \r\n",
        "    \r\n",
        "    def transform(self, X, y=None):\r\n",
        "      for i in range(len(self.cat_cols)):\r\n",
        "        #Temporary measure to handle previously unseen values\r\n",
        "        #replace unseen values with NaN\r\n",
        "        X.loc[X[~X[(str(self.cat_cols[i]))].isin(self.categories[i])].index,(str(self.cat_cols[i]))]=np.NaN\r\n",
        "\r\n",
        "        #replace seen values with encoding\r\n",
        "        X.loc[:,(str(self.cat_cols[i]))].replace(dict(zip(self.categories[i], self.RSE[i])),inplace=True)\r\n",
        "      return X    \r\n",
        "\r\n",
        "    def inverse_transform(self,X):\r\n",
        "      for i in range(len(self.cat_cols)):\r\n",
        "        X.loc[:,(str(self.cat_cols[i]))].replace(dict(zip(self.RSE[i], self.categories[i])),inplace=True)\r\n",
        "      return X"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx7OsDXbDKSZ"
      },
      "source": [
        "#custom transformer for incrementally scaling to standard scale using pooled mean and variance\r\n",
        "class CustomStandardScaler(BaseEstimator, TransformerMixin):\r\n",
        "  def __init__(self,mean=None,var=None,n_samples_seen=None,scale=None):\r\n",
        "    self.mean=None #mean\r\n",
        "    self.var=None\r\n",
        "    self.n_samples_seen=None\r\n",
        "    self.scale=None\r\n",
        "\r\n",
        "  def compute_sample_mean(self,X):\r\n",
        "    return np.mean(X,axis=0)\r\n",
        "\r\n",
        "  def compute_sample_var(self,X):\r\n",
        "    return np.var(X,axis=0)\r\n",
        "\r\n",
        "  def compute_sample_size(self,X):\r\n",
        "    #assuming X is imputed, if there are null values, throw error aksing that X be imputed first\r\n",
        "    return len(X)\r\n",
        "\r\n",
        "  def compute_pooled_mean(self,X):\r\n",
        "    #compute the sample mean and size\r\n",
        "    sample_mean=self.compute_sample_mean(X)\r\n",
        "    sample_count=self.compute_sample_size(X) \r\n",
        "    #compute pool mean\r\n",
        "    pool_mean=(self.mean*self.n_samples_seen + sample_mean*sample_count)/(self.n_samples_seen + sample_count)\r\n",
        "\r\n",
        "    return pool_mean\r\n",
        "\r\n",
        "  def compute_pooled_var(self,X):\r\n",
        "    #compute the sample var and size\r\n",
        "    sample_var=self.compute_sample_var(X)\r\n",
        "    sample_count=self.compute_sample_size(X) \r\n",
        "    #compute pool variance\r\n",
        "    pool_var=(self.var*(self.n_samples_seen - 1) + sample_var*(sample_count - 1))/(self.n_samples_seen + sample_count - 2)\r\n",
        "\r\n",
        "    return pool_var\r\n",
        "\r\n",
        "  def fit(self,X):\r\n",
        "    if self.mean is None:\r\n",
        "      self.mean=self.compute_sample_mean(X)\r\n",
        "    else: \r\n",
        "      self.mean=self.compute_pooled_mean(X)\r\n",
        "    \r\n",
        "    if self.var is None:\r\n",
        "      self.var=self.compute_sample_var(X)\r\n",
        "    else: \r\n",
        "      self.var=self.compute_pooled_var(X)\r\n",
        "\r\n",
        "    if self.n_samples_seen is None:\r\n",
        "      self.n_samples_seen=self.compute_sample_size(X) \r\n",
        "    else: \r\n",
        "      self.n_samples_seen+=self.compute_sample_size(X)\r\n",
        "    return self\r\n",
        "\r\n",
        "  def transform(self,X):\r\n",
        "    return (X-self.mean)/np.sqrt(self.var)\r\n",
        "\r\n",
        "  def inverse_transform(self,X):\r\n",
        "    return X*np.sqrt(self.var) + self.mean\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRSCJIJwC0jq"
      },
      "source": [
        "#Import Q1 and Q2 data into 2 separate dataframes \r\n",
        "required_features=read_csv_to_list('https://raw.githubusercontent.com/sharsulkar/H1B_LCA_outcome_prediction/main/data/processed/required_features.csv',header=None,squeeze=True)\r\n",
        "\r\n",
        "data1_df=pd.read_excel('/content/drive/MyDrive/Datasets/H1B_LCA_prediction/LCA_Disclosure_Data_FY2020_Q1.xlsx',usecols=required_features)\r\n",
        "data1_dfcopy=data1_df.copy()\r\n",
        "\r\n",
        "data2_df=pd.read_excel('/content/drive/MyDrive/Datasets/H1B_LCA_prediction/LCA_Disclosure_Data_FY2020_Q2.xlsx',usecols=required_features)\r\n",
        "data2_dfcopy=data1_df.copy()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNE-z47C2sx"
      },
      "source": [
        "fe_cols=read_csv_to_list('https://raw.githubusercontent.com/sharsulkar/H1B_LCA_outcome_prediction/main/data/processed/feature_engineering_columns.csv',header=None,squeeze=True)\r\n",
        "drop_cols=read_csv_to_list('https://github.com/sharsulkar/H1B_LCA_outcome_prediction/raw/main/data/processed/drop_columns.csv',header=None,squeeze=True)\r\n",
        "cat_cols=read_csv_to_list('https://raw.githubusercontent.com/sharsulkar/H1B_LCA_outcome_prediction/main/data/processed/categorical_columns.csv',header=None,squeeze=True)\r\n",
        "num_cols=read_csv_to_list('https://raw.githubusercontent.com/sharsulkar/H1B_LCA_outcome_prediction/main/data/processed/numeric_columns.csv',header=None,squeeze=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQDbMjtDNYh"
      },
      "source": [
        "#Build preprocessing pipeline\r\n",
        "build_feature_pipe=make_pipeline(\r\n",
        "    droprows_Transformer(),\r\n",
        "    buildfeatures_Transformer(fe_cols)\r\n",
        "    )\r\n",
        "\r\n",
        "numerical_preprocess=make_pipeline(\r\n",
        "    SimpleImputer(strategy='median'),\r\n",
        "    CustomStandardScaler()\r\n",
        ")\r\n",
        "preprocess_pipe=make_column_transformer(\r\n",
        "    (dropfeatures_Transformer(columns=drop_cols,inplace=True),drop_cols),\r\n",
        "    (RSE_Transformer(cat_cols),cat_cols),\r\n",
        "    (numerical_preprocess,num_cols),\r\n",
        "    remainder='passthrough'\r\n",
        ")\r\n",
        "all_preprocess=make_pipeline(\r\n",
        "    preprocess_pipe\r\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmG9C4snDZqh"
      },
      "source": [
        "#build features + drop rows where CASE_STATUs not in ['Certified','Denied']\r\n",
        "fe_df=build_feature_pipe.fit_transform(data2_df)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxwKtTeVE4zy"
      },
      "source": [
        "#separate the target variable and encode\r\n",
        "y=fe_df.pop('CASE_STATUS')\r\n",
        "y.replace(['Certified','Denied'],[0,1],inplace=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRklH-N8FM8i"
      },
      "source": [
        "#apply remaining preprocess pipeline to the semi processed dataframe\r\n",
        "X=all_preprocess.fit_transform(fe_df)\r\n",
        "print(X.shape)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXLc1Uh-Fqa4"
      },
      "source": [
        "#save the build_feature_pipe and preprocess pipelines\r\n",
        "dump(all_preprocess,open('/content/drive/MyDrive/saved_models/H1B_LCA_prediction/pipeline_batch_train.pkl','wb'))\r\n",
        "dump(build_feature_pipe,open('/content/drive/MyDrive/saved_models/H1B_LCA_prediction/build_feature_pipe_batch_train.pkl','wb'))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX4NLG7XHZio"
      },
      "source": [
        "#instantiate model\r\n",
        "adaboost=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=100),n_estimators=200,learning_rate=0.1,random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO8WGlZyFRps",
        "outputId": "f93d8a37-1fd5-4e5b-ca99-168f5aba6a7f"
      },
      "source": [
        "#fit model\r\n",
        "adaboost.fit(X,y)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                         class_weight=None,\n",
              "                                                         criterion='gini',\n",
              "                                                         max_depth=100,\n",
              "                                                         max_features=None,\n",
              "                                                         max_leaf_nodes=None,\n",
              "                                                         min_impurity_decrease=0.0,\n",
              "                                                         min_impurity_split=None,\n",
              "                                                         min_samples_leaf=1,\n",
              "                                                         min_samples_split=2,\n",
              "                                                         min_weight_fraction_leaf=0.0,\n",
              "                                                         presort='deprecated',\n",
              "                                                         random_state=None,\n",
              "                                                         splitter='best'),\n",
              "                   learning_rate=0.1, n_estimators=200, random_state=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsWalOmHGZ-5"
      },
      "source": [
        "#save the model\r\n",
        "dump(adaboost,open('/content/drive/MyDrive/saved_models/H1B_LCA_prediction/adaboost_batch_train.pkl','wb'))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9LtdoDvGoca",
        "outputId": "c4873def-b1b1-4d3d-ebde-4ab703726c6d"
      },
      "source": [
        "#check training metrics just to make sure model has fitted\r\n",
        "print(f1_score(y_true=y,y_pred=adaboost.predict(X),average=None))\r\n",
        "confusion_matrix(y_true=y,y_pred=adaboost.predict(X))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.99993667 0.9919865 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[150002,      9],\n",
              "       [    10,   1176]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4rBDoAXNcVO",
        "outputId": "94dc57c8-73dc-4f7a-d960-ca12a2a23397"
      },
      "source": [
        "all_preprocess"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('columntransformer',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='passthrough',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('dropfeatures_transformer',\n",
              "                                                  dropfeatures_Transformer(columns=['BEGIN_DATE',\n",
              "                                                                                    'PW_UNIT_OF_PAY',\n",
              "                                                                                    'END_DATE',\n",
              "                                                                                    'RECEIVED_DATE',\n",
              "                                                                                    'DECISION_DATE',\n",
              "                                                                                    'PREVAILING_WAGE',\n",
              "                                                                                    'WORKSITE_POSTAL_CODE',\n",
              "                                                                                    'PW_OES_YEA...\n",
              "                                                                                        n_samples_seen=None,\n",
              "                                                                                        scale=None,\n",
              "                                                                                        var=None))],\n",
              "                                                           verbose=False),\n",
              "                                                  ['TOTAL_WORKER_POSITIONS',\n",
              "                                                   'NEW_EMPLOYMENT',\n",
              "                                                   'CONTINUED_EMPLOYMENT',\n",
              "                                                   'CHANGE_PREVIOUS_EMPLOYMENT',\n",
              "                                                   'NEW_CONCURRENT_EMPLOYMENT',\n",
              "                                                   'CHANGE_EMPLOYER',\n",
              "                                                   'AMENDED_PETITION',\n",
              "                                                   'WORKSITE_WORKERS',\n",
              "                                                   'TOTAL_WORKSITE_LOCATIONS',\n",
              "                                                   'PROCESSING_DAYS',\n",
              "                                                   'VALIDITY_DAYS',\n",
              "                                                   'WAGE_ABOVE_PW_HR'])],\n",
              "                                   verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}