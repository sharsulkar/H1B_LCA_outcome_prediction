{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_sh_vizualizations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VLl3_j-uVq3DMLewFsZvX3zqff2qt060",
      "authorship_tag": "ABX9TyOrzJSpSqkiZthJAxWcp6WW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharsulkar/H1B_LCA_outcome_prediction/blob/main/prototyping/notebooks/05_sh_vizualizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUl47qrH1KR3"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pickle import dump, load\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKinMQUsiaU4"
      },
      "source": [
        "#Custom transformer to drop rows based on filter\n",
        "class droprows_Transformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "      self.row_index = None # row index to drop\n",
        "      self.inplace=True\n",
        "      self.reset_index=True\n",
        "\n",
        "    def fit( self, X, y=None):\n",
        "      return self \n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "      self.row_index=X[~X.CASE_STATUS.isin(['Certified','Denied'])].index\n",
        "      X.drop(index=self.row_index,inplace=self.inplace)\n",
        "      if self.reset_index:\n",
        "        X.reset_index(inplace=True,drop=True)\n",
        "      return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERTQaplWicZf"
      },
      "source": [
        "class buildfeatures_Transformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, input_columns):\n",
        "    self.input_columns=input_columns\n",
        "\n",
        "  def date_diff(self,date1,date2):\n",
        "    return date1-date2\n",
        "\n",
        "  def is_USA(self,country):\n",
        "    if country=='UNITED STATES OF AMERICA':\n",
        "      USA_YN='Y' \n",
        "    else:\n",
        "      USA_YN='N'\n",
        "    return USA_YN\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    # Processing_Days and Validity_days\n",
        "    X['PROCESSING_DAYS']=self.date_diff(X.DECISION_DATE, X.RECEIVED_DATE).dt.days\n",
        "    X['VALIDITY_DAYS']=self.date_diff(X.END_DATE, X.BEGIN_DATE).dt.days\n",
        "\n",
        "    # SOC_Codes\n",
        "    X['SOC_CD2']=X.SOC_CODE.str.split(pat='-',n=1,expand=True)[0]\n",
        "    X['SOC_CD4']=X.SOC_CODE.str.split(pat='-',n=1,expand=True)[1].str.split(pat='.',n=1,expand=True)[0]\n",
        "    X['SOC_CD_ONET']=X.SOC_CODE.str.split(pat='-',n=1,expand=True)[1].str.split(pat='.',n=1,expand=True)[1]\n",
        "\n",
        "    # USA_YN\n",
        "    X['USA_YN']=X.EMPLOYER_COUNTRY.apply(self.is_USA)\n",
        "\n",
        "    # Employer_Worksite_YN\n",
        "    X['EMPLOYER_WORKSITE_YN']='Y'\n",
        "    X.loc[X.EMPLOYER_POSTAL_CODE.ne(X.WORKSITE_POSTAL_CODE),'EMPLOYER_WORKSITE_YN']='N'\n",
        "\n",
        "    # OES_YN\n",
        "    X['OES_YN']='Y'\n",
        "    X.iloc[X[~X.PW_OTHER_SOURCE.isna()].index,X.columns.get_loc('OES_YN')]='N'\n",
        "\n",
        "    # SURVEY_YEAR\n",
        "    X['SURVEY_YEAR']=pd.to_datetime(X.PW_OES_YEAR.str.split(pat='-',n=1,expand=True)[0]).dt.to_period('Y')\n",
        "    PW_other_year=X[X.OES_YN=='N'].PW_OTHER_YEAR\n",
        "    #Rename the series and update dataframe with series object\n",
        "    PW_other_year.rename(\"SURVEY_YEAR\",inplace=True)\n",
        "    X.update(PW_other_year)\n",
        "\n",
        "    # WAGE_ABOVE_PREVAILING_HR\n",
        "    X['WAGE_PER_HR']=X.WAGE_RATE_OF_PAY_FROM\n",
        "    #compute for Year\n",
        "    X.iloc[X[X.WAGE_UNIT_OF_PAY=='Year'].index,X.columns.get_loc('WAGE_PER_HR')]=X[X.WAGE_UNIT_OF_PAY=='Year'].WAGE_RATE_OF_PAY_FROM/2067\n",
        "    #compute for Month\n",
        "    X.iloc[X[X.WAGE_UNIT_OF_PAY=='Month'].index,X.columns.get_loc('WAGE_PER_HR')]=X[X.WAGE_UNIT_OF_PAY=='Month'].WAGE_RATE_OF_PAY_FROM/172\n",
        "\n",
        "    #initialize with WAGE_RATE_OF_PAY_FROM\n",
        "    X['PW_WAGE_PER_HR']=X.PREVAILING_WAGE\n",
        "    #compute for Year\n",
        "    X.iloc[X[X.PW_UNIT_OF_PAY=='Year'].index,X.columns.get_loc('PW_WAGE_PER_HR')]=X[X.PW_UNIT_OF_PAY=='Year'].PREVAILING_WAGE/2067\n",
        "    #compute for Month\n",
        "    X.iloc[X[X.PW_UNIT_OF_PAY=='Month'].index,X.columns.get_loc('PW_WAGE_PER_HR')]=X[X.PW_UNIT_OF_PAY=='Month'].PREVAILING_WAGE/172\n",
        "\n",
        "    X['WAGE_ABOVE_PW_HR']=X.WAGE_PER_HR-X.PW_WAGE_PER_HR\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q3zpeHxifSh"
      },
      "source": [
        "#Custom transformer to drop features for input feature list\n",
        "class dropfeatures_Transformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns, inplace):\n",
        "      self.columns = columns # list of categorical columns in input Dataframe\n",
        "      self.inplace=True\n",
        "\n",
        "    def fit( self, X, y=None):\n",
        "      return self \n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "      X.drop(columns=self.columns,inplace=self.inplace)\n",
        "      return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL0nz2vNihf4"
      },
      "source": [
        "#Custom transformer to compute Random Standard encoding for categorical features for incrementaly encoding data\n",
        "class RSE_Transformer(BaseEstimator, TransformerMixin):\n",
        "    #Class Constructor\n",
        "    def __init__( self, cat_cols, categories=None, RSE=None ):\n",
        "        self.cat_cols = cat_cols # list of categorical columns in input Dataframe\n",
        "        self.categories = None # Array of unique non-numeric values in each categorical column\n",
        "        self.RSE = None # Array of Random Standard encoding for each row in categories\n",
        "        \n",
        "    def fit( self, X, y=None ):\n",
        "      #Get a list of all unique categorical values for each column\n",
        "      if self.categories is None:\n",
        "        self.categories = [X[column].unique() for column in cat_cols]\n",
        "\n",
        "        #replace missing values and append missing value label to each column to handle missing values in test dataset that might not be empty in train dataset\n",
        "        for i in range(len(self.categories)):\n",
        "          if np.array(self.categories[i].astype(str)!=str(np.nan)).all():\n",
        "            self.categories[i]=np.append(self.categories[i],np.nan)\n",
        "\n",
        "        #compute RandomStandardEncoding \n",
        "        self.RSE=[np.random.normal(0,1,len(self.categories[i])) for i in range(len(self.cat_cols))]\n",
        "\n",
        "      else:\n",
        "        for i in range(len(self.cat_cols)):\n",
        "          #append new unique categories to self.categories\n",
        "          new_categories=list(set(X[self.cat_cols[i]].unique()).difference(set(self.categories[i])))\n",
        "          if new_categories!=[]:\n",
        "            #print('not empty') #replace with logging call\n",
        "            #print('categories before append',len(categories[i])) #logging call\n",
        "            self.categories[i]=np.append(self.categories[i],new_categories) #append new categories to the end\n",
        "            new_RSE=np.random.normal(0,1,len(new_categories)) #generate new RSE values\n",
        "            #regenrate if overlap found with existing encodings\n",
        "            if set(new_RSE).issubset(set(self.RSE[i])): \n",
        "              #print('yes') #loggin call\n",
        "              new_RSE=np.random.normal(0,1,len(new_categories))\n",
        "            \n",
        "            self.RSE[i]=np.append(self.RSE[i],new_RSE) #append new RSE values\n",
        "          #print('new categories',len(new_categories)) #logging call\n",
        "          #print('categories after append',len(categories[i]))\n",
        "     \n",
        "      return self \n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "      for i in range(len(self.cat_cols)):\n",
        "        #Temporary measure to handle previously unseen values\n",
        "        #replace unseen values with NaN\n",
        "        X.loc[X[~X[(str(self.cat_cols[i]))].isin(self.categories[i])].index,(str(self.cat_cols[i]))]=np.NaN\n",
        "\n",
        "        #replace seen values with encoding\n",
        "        X.loc[:,(str(self.cat_cols[i]))].replace(dict(zip(self.categories[i], self.RSE[i])),inplace=True)\n",
        "      return X    \n",
        "\n",
        "    def inverse_transform(self,X):\n",
        "      for i in range(len(self.cat_cols)):\n",
        "        X.loc[:,(str(self.cat_cols[i]))].replace(dict(zip(self.RSE[i], self.categories[i])),inplace=True)\n",
        "      return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IPqIgp1ijr3"
      },
      "source": [
        "#custom transformer for incrementally scaling to standard scale using pooled mean and variance\n",
        "class CustomStandardScaler(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self,mean=None,var=None,n_samples_seen=None,scale=None):\n",
        "    self.mean=None #mean\n",
        "    self.var=None\n",
        "    self.n_samples_seen=None\n",
        "    self.scale=None\n",
        "\n",
        "  def compute_sample_mean(self,X):\n",
        "    return np.mean(X,axis=0)\n",
        "\n",
        "  def compute_sample_var(self,X):\n",
        "    return np.var(X,axis=0)\n",
        "\n",
        "  def compute_sample_size(self,X):\n",
        "    #assuming X is imputed, if there are null values, throw error aksing that X be imputed first\n",
        "    return len(X)\n",
        "\n",
        "  def compute_pooled_mean(self,X):\n",
        "    #compute the sample mean and size\n",
        "    sample_mean=self.compute_sample_mean(X)\n",
        "    sample_count=self.compute_sample_size(X) \n",
        "    #compute pool mean\n",
        "    pool_mean=(self.mean*self.n_samples_seen + sample_mean*sample_count)/(self.n_samples_seen + sample_count)\n",
        "\n",
        "    return pool_mean\n",
        "\n",
        "  def compute_pooled_var(self,X):\n",
        "    #compute the sample var and size\n",
        "    sample_var=self.compute_sample_var(X)\n",
        "    sample_count=self.compute_sample_size(X) \n",
        "    #compute pool variance\n",
        "    pool_var=(self.var*(self.n_samples_seen - 1) + sample_var*(sample_count - 1))/(self.n_samples_seen + sample_count - 2)\n",
        "\n",
        "    return pool_var\n",
        "\n",
        "  def fit(self,X):\n",
        "    if self.mean is None:\n",
        "      self.mean=self.compute_sample_mean(X)\n",
        "    else: \n",
        "      self.mean=self.compute_pooled_mean(X)\n",
        "    \n",
        "    if self.var is None:\n",
        "      self.var=self.compute_sample_var(X)\n",
        "    else: \n",
        "      self.var=self.compute_pooled_var(X)\n",
        "\n",
        "    if self.n_samples_seen is None:\n",
        "      self.n_samples_seen=self.compute_sample_size(X) \n",
        "    else: \n",
        "      self.n_samples_seen+=self.compute_sample_size(X)\n",
        "    return self\n",
        "\n",
        "  def transform(self,X):\n",
        "    return (X-self.mean)/np.sqrt(self.var)\n",
        "\n",
        "  def inverse_transform(self,X):\n",
        "    return X*np.sqrt(self.var) + self.mean\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gQuWA9wimS7"
      },
      "source": [
        "def read_csv_to_list(filepath,header=None,squeeze=True):\n",
        "  return list(pd.read_csv(filepath,header=None,squeeze=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-8M1Xnn1jI_"
      },
      "source": [
        "df=pd.read_excel('/content/drive/MyDrive/Datasets/LCA_dataset_sample1000.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__qmiVxEYz8N"
      },
      "source": [
        "model=load(open('/content/drive/MyDrive/saved_models/H1B_LCA_prediction/adaboost_batch_train.pkl','rb'))\n",
        "build_feature_pipe=load(open('/content/drive/MyDrive/saved_models/H1B_LCA_prediction/build_feature_pipe_batch_train.pkl','rb'))\n",
        "preprocess_pipe=load(open('/content/drive/MyDrive/saved_models/H1B_LCA_prediction/pipeline_batch_train.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QoIce-22K0H"
      },
      "source": [
        "fe_df=build_feature_pipe.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwXcw_QU4852"
      },
      "source": [
        "#Display outcome statistics for -\r\n",
        "#Specific to the test sample -\r\n",
        "#employer name - stats for current employer\r\n",
        "#NAICS code - current Naics code\r\n",
        "#SOC_Code - current Soc code\r\n",
        "\r\n",
        "#Generic stats from training data-\r\n",
        "#visa class, Wage level, AGREE_TO_LC_STATEMENT, H-1B_DEPENDENT, WILLFUL_VIOLATOR, PUBLIC_DISCLOSURE, EMPLOYER_WORKSITE_YN\r\n",
        "#validity days=3y vs <3y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbfKNNP_nMzu"
      },
      "source": [
        "def generate_feature_stats(df,column,stats_file_path=None):\n",
        "  if stats_file_path is None:\n",
        "    #create new file\n",
        "    stats_df=pd.DataFrame(index=np.unique(df[column]),columns=['Certified','Denied'])\n",
        "    for index in stats_df.index:\n",
        "      stats_df.loc[index,['Certified']]=df[(df[column]==index) & (df.CASE_STATUS=='Certified')].shape[0]\n",
        "      stats_df.loc[index,['Denied']]=df[(df[column]==index) & (df.CASE_STATUS=='Denied')].shape[0]\n",
        "\n",
        "  else:\n",
        "    #import the existing file and append results\n",
        "    stat_df=pd.read_csv(stats_file_path,index=0)\n",
        "    new_index=np.unique(df[column])\n",
        "    for index in new_index:\n",
        "      if index in stat_df.index: #rewrite with correct condition\n",
        "        stats_df.loc[index,['Certified']]+=df[(df[column]==index) & (df.CASE_STATUS=='Certified')].shape[0]\n",
        "        stats_df.loc[index,['Denied']]+=df[(df[column]==index) & (df.CASE_STATUS=='Denied')].shape[0]\n",
        "      else:\n",
        "        stats_df.loc[index,['Certified']]=df[(df[column]==index) & (df.CASE_STATUS=='Certified')].shape[0]\n",
        "        stats_df.loc[index,['Denied']]=df[(df[column]==index) & (df.CASE_STATUS=='Denied')].shape[0]\n",
        "  \n",
        "  #save the df as csv\n",
        "  return stats_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elEA1FDdr6yn"
      },
      "source": [
        "def compute_stats_positive_class(column,value,stats_file_path=None):\n",
        "  #import the file into a df\n",
        "  stat_df=pd.read_csv(stats_file_path,index=0)\n",
        "  #find the index\n",
        "  stats_arr=np.array(stats_df.loc[index])\n",
        "  #return the \n",
        "  return stats_arr[0]*100/stats_arr.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzkiq6MnNUJf"
      },
      "source": [
        "#if prediction is denied, create a grid of all possible values of variable features, run them through the model and display those that return a positive outcome as suggestions \r\n",
        "#constant feature list - VISA_CLASS, EMPLOYER_NAME, EMPLOYER_POSTAL_CODE, EMPLOYER_COUNTRY, NAICS_CODE, SECONDARY_ENTITY, PREVAILING_WAGE, SOC_CODE, SOC_TITLE, SURVEY RELATED COLUMNS, \r\n",
        "#variable features list - FULL_TIME_POSITION, PW_WAGE_LEVEL,  AGREE_TO_LC_STATEMENT, H-1B_DEPENDENT, WILLFUL_VIOLATOR, PUBLIC_DISCLOSURE, NEW_EMPLOYMENT, CONTINUED_EMPLOYMENT, CHANGE_PREVIOUS_EMPLOYMENT, NEW_CONCURRENT_EMPLOYMENT, CHANGE_EMPLOYER,\r\n",
        "#AMENDED_PETITION, AGENT_REPRESENTING_EMPLOYER, EMPLOYER_WORKSITE_YN, WAGE_ABOVE_PW_HR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU-z3ZGBuMWU"
      },
      "source": [
        "def return_allcombinations(arr):\n",
        "  #calculate dimensions of final array\n",
        "  cum_prod=np.cumprod([len(arr[i]) for i in range(len(arr))])\n",
        "  m=np.prod([len(arr[i]) for i in range(len(arr))])\n",
        "  hlayer_arr=np.array(arr[0]).repeat(m/cum_prod[0])\n",
        "  for i in range(1,len(arr)):\n",
        "      cc=np.array(arr[i]).repeat(m/cum_prod[i])\n",
        "\n",
        "      for j in range(np.int(cum_prod[i-1])-1):\n",
        "        cc=np.hstack((cc,np.array(arr[i]).repeat(m/cum_prod[i])))\n",
        "      hlayer_arr=np.vstack((hlayer_arr,cc))\n",
        "  return hlayer_arr.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KreXjWedOm7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9ef1bd-92a4-4aed-e0ff-1854cf9c3e3f"
      },
      "source": [
        "denied_df=fe_df[fe_df.CASE_STATUS=='Denied'].reset_index(drop=True)\n",
        "denied_df.pop('CASE_STATUS')\n",
        "#user input record that needs to be predicted\n",
        "X_pred=pd.DataFrame(denied_df.iloc[7]).T\n",
        "X_arr=preprocess_pipe.transform(X_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BOycC30vLl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3627a250-5717-4650-bead-8a74090c9907"
      },
      "source": [
        "#define variable column list that will be used for grid search\n",
        "#create array of unique values for all above columns sourcing from the RSE transform\n",
        "#arr=[['Y','N'],['I','II','III','IV'],['Y','N'],['Y','N'],['Y','N'],['Disclose Business', 'Disclose Employment', 'Disclose Business and Employment'],['Y','N'],['Y','N'],['Y','N'],[0,10],[100,500,1095]]\n",
        "arr=[['Y','N'],['I','II','III','IV'],['Y','N'],['Y','N'],['Y','N'],['Disclose Business', 'Disclose Employment','Disclose Business and Employment'],['Y','N'],['Y','N'],['Y','N']]\n",
        "#append wage information\n",
        "wage_arr=[0,10]\n",
        "if X_pred.WAGE_ABOVE_PW_HR.values[0]>0:\n",
        "  wage_arr=np.append(wage_arr,X_pred.WAGE_ABOVE_PW_HR.values[0])\n",
        "arr.append(list(wage_arr))\n",
        "#append validity days information\n",
        "validity_days_arr=[100,1095]\n",
        "if X_pred.VALIDITY_DAYS.values[0]<1095:\n",
        "  validity_days_arr=np.append(validity_days_arr,X_pred.VALIDITY_DAYS.values[0])\n",
        "arr.append(list(validity_days_arr))\n",
        "\n",
        "#create grid array for all possible combinations\n",
        "grid_arr=return_allcombinations(arr)\n",
        "grid_len=grid_arr.shape[0]\n",
        "\n",
        "#generate base grid dataframe by repeating the X_pred sample grid_len times\n",
        "X_reconstructed=X_pred.iloc[np.arange(1).repeat(grid_len)].reset_index(drop=True)\n",
        "\n",
        "#update the grid dataframe with the grid array\n",
        "var_columns=['FULL_TIME_POSITION', 'PW_WAGE_LEVEL',  'AGREE_TO_LC_STATEMENT', 'H-1B_DEPENDENT', 'WILLFUL_VIOLATOR', 'PUBLIC_DISCLOSURE', 'AGENT_REPRESENTING_EMPLOYER','EMPLOYER_WORKSITE_YN', 'OES_YN','WAGE_ABOVE_PW_HR','VALIDITY_DAYS']\n",
        "pd.DataFrame.update(X_reconstructed,pd.DataFrame(grid_arr,columns=var_columns))\n",
        "\n",
        "#apply pipeline to the recon\n",
        "X_reconstructed_arr=preprocess_pipe.transform(X_reconstructed)\n",
        "\n",
        "y=model.predict(X_reconstructed_arr)\n",
        "len(y[y==0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13696"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "9qGsMSaJ4Opm",
        "outputId": "dbab8cba-5ad2-47c3-c89f-d5c8ed604d69"
      },
      "source": [
        "cat_cols=read_csv_to_list('https://raw.githubusercontent.com/sharsulkar/H1B_LCA_outcome_prediction/main/data/processed/categorical_columns.csv',header=None,squeeze=True)\n",
        "num_cols=read_csv_to_list('https://raw.githubusercontent.com/sharsulkar/H1B_LCA_outcome_prediction/main/data/processed/numeric_columns.csv',header=None,squeeze=True)\n",
        "print(np.append(cat_cols,num_cols))\n",
        "'''\n",
        "#column indices of features of intrest after dropfeatures_Transformer\n",
        "['FULL_TIME_POSITION' - 3,  \n",
        " 'AGENT_REPRESENTING_EMPLOYER' - 5, \n",
        " 'PW_WAGE_LEVEL' - 7,\n",
        " 'AGREE_TO_LC_STATEMENT'-8, \n",
        " 'H-1B_DEPENDENT'-9,\n",
        " 'WILLFUL_VIOLATOR'-10,\n",
        " 'PUBLIC_DISCLOSURE'-11,\n",
        " 'EMPLOYER_WORKSITE_YN'-16, \n",
        " 'OES_YN' - 17,\n",
        " 'VALIDITY_DAYS'-29,\n",
        " 'WAGE_ABOVE_PW_HR' - 30]\n",
        " '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['VISA_CLASS' 'SOC_TITLE' 'EMPLOYER_NAME' 'FULL_TIME_POSITION'\n",
            " 'NAICS_CODE' 'AGENT_REPRESENTING_EMPLOYER' 'SECONDARY_ENTITY'\n",
            " 'PW_WAGE_LEVEL' 'AGREE_TO_LC_STATEMENT' 'H-1B_DEPENDENT'\n",
            " 'WILLFUL_VIOLATOR' 'PUBLIC_DISCLOSURE' 'SOC_CD2' 'SOC_CD4' 'SOC_CD_ONET'\n",
            " 'USA_YN' 'EMPLOYER_WORKSITE_YN' 'OES_YN' 'SURVEY_YEAR'\n",
            " 'TOTAL_WORKER_POSITIONS' 'NEW_EMPLOYMENT' 'CONTINUED_EMPLOYMENT'\n",
            " 'CHANGE_PREVIOUS_EMPLOYMENT' 'NEW_CONCURRENT_EMPLOYMENT'\n",
            " 'CHANGE_EMPLOYER' 'AMENDED_PETITION' 'WORKSITE_WORKERS'\n",
            " 'TOTAL_WORKSITE_LOCATIONS' 'PROCESSING_DAYS' 'VALIDITY_DAYS'\n",
            " 'WAGE_ABOVE_PW_HR']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#column indices of features of intrest after dropfeatures_Transformer\\n['FULL_TIME_POSITION' - 3,  \\n 'AGENT_REPRESENTING_EMPLOYER' - 5, \\n 'PW_WAGE_LEVEL' - 7,\\n 'AGREE_TO_LC_STATEMENT'-8, \\n 'H-1B_DEPENDENT'-9,\\n 'WILLFUL_VIOLATOR'-10,\\n 'PUBLIC_DISCLOSURE'-11,\\n 'EMPLOYER_WORKSITE_YN'-16, \\n 'OES_YN' - 17,\\n 'VALIDITY_DAYS'-29,\\n 'WAGE_ABOVE_PW_HR' - 30]\\n \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDQY2DojZm93"
      },
      "source": [
        "def weightedL2(a, b, w):\r\n",
        "    q = a-b\r\n",
        "    return np.sqrt((w*q*q).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9YY3aY54cRp"
      },
      "source": [
        "#set weights to one\r\n",
        "w=np.ones(len(X_arr[0])) #initialize weights with 1\r\n",
        "\r\n",
        "#change weights of some fields to give them priority over others\r\n",
        "variable_idx=[3,5,7,8,9,10,11,16,17,29,30] # index of variable fields where changes in values can impact outcome\r\n",
        "w[variable_idx]=[1,1,2,1,1,1,1,1,1,2,2] #increase weights of PW_WAGE_LEVEL,VALIDITY_DAYS,WAGE_ABOVE_PW_HR as those as most easy to change for the user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnMS6VYm-iCa",
        "outputId": "86d51938-f81f-43d9-9845-b6581dcb5ab5"
      },
      "source": [
        "if len(y[y==0])>0: #check if any of the combination will result in a confirmation\n",
        "\n",
        "  #Calculate weighted eucledian distance between the X_pred and each row in grid and return index of minimum distance\n",
        "  distance_arr=[]\n",
        "  for i in range(grid_len):\n",
        "    if y[i]==0:\n",
        "      #q=X_arr[0]-X_reconstructed_arr[i]\n",
        "      #L2_distance=np.sqrt((w*q*q).sum())\n",
        "      L2_distance=weightedL2(X_arr[0], X_reconstructed_arr[i], w)\n",
        "      if L2_distance==0: #distance between X_pred and itself in the grid is 0\n",
        "        L2_distance+=1 #add offset so it does not return the same record if it is misclassified to class 0\n",
        "      distance_arr=np.append(distance_arr,L2_distance) #append the distance\n",
        "\n",
        "  #find the first suggesting with minimum changes to the application\n",
        "  min_change_index=np.argmin(distance_arr)\n",
        "  #np.argsort(distance_arr)[:3] #return indices for min 3 records\n",
        "\n",
        "  print('The model predicts that application\\'s outcome can change from Denied to Confirmed if all the below changes are made:')\n",
        "  #print the suggested changes that will result in a approved application\n",
        "  for index in np.nonzero(X_arr[0] - X_reconstructed_arr[min_change_index])[0]:\n",
        "    #print(index)\n",
        "    \n",
        "    feature_name=np.append(cat_cols,num_cols)[index]\n",
        "    current_value=X_pred[feature_name].iloc[0]\n",
        "    new_value=X_reconstructed[feature_name].iloc[min_change_index]\n",
        "    #replace nan with missing\n",
        "    if current_value==np.NaN:\n",
        "       current_value='missing'\n",
        "    #handle specific features\n",
        "    if (feature_name=='VALIDITY_DAYS' and current_value>=1094):\n",
        "      #dont suggest reducing validity days, more the better\n",
        "      continue\n",
        "\n",
        "    elif feature_name=='WAGE_ABOVE_PW_HR':\n",
        "      if current_value>float(new_value):\n",
        "      #dont suggest reducing the wage\n",
        "        continue\n",
        "      else:\n",
        "        print('Consider Increasing the per hour wage rate by',new_value,' over the given per hour prewailing wage.')\n",
        "\n",
        "    elif feature_name=='EMPLOYER_WORKSITE_YN':\n",
        "      if current_value=='Y':\n",
        "      #dont suggest reducing the wage\n",
        "        print('Consider changing the worksite location to a postal code other than the employer address.')\n",
        "      else:\n",
        "        print('Consider changing the worksite location to a postal code same as the employer address.')\n",
        "\n",
        "    elif feature_name=='OES_YN':\n",
        "      #OES_YN is a derived feature, give explaination that end user can understand\n",
        "      if current_value=='Y':\n",
        "        print('Consider using survey other than OES.')\n",
        "      else:\n",
        "        print('Consider using OES survey.')\n",
        "    \n",
        "    else:\n",
        "      print('Consider changing ',feature_name, ' from ',current_value,' to ',new_value)\n",
        "else:\n",
        "  print('We are sorry. The model does not predict a change in your application outcome with current parameters. Please check the documentation for the complete list of parameters used.')\n",
        "  print('Please consider consulting with a qualified immigration professional regarding your application.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model predicts that application's outcome can change from Denied to Confirmed if all the below changes are made:\n",
            "Consider changing  PW_WAGE_LEVEL  from  nan  to  I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16B-bsmqgf14",
        "outputId": "f8e9d446-2aa6-4278-91e6-bfef7e99f274"
      },
      "source": [
        "denied_df[var_columns].iloc[7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FULL_TIME_POSITION                             Y\n",
              "PW_WAGE_LEVEL                                NaN\n",
              "AGREE_TO_LC_STATEMENT                          Y\n",
              "H-1B_DEPENDENT                                 N\n",
              "WILLFUL_VIOLATOR                               N\n",
              "PUBLIC_DISCLOSURE              Disclose Business\n",
              "AGENT_REPRESENTING_EMPLOYER                    Y\n",
              "EMPLOYER_WORKSITE_YN                           Y\n",
              "OES_YN                                         N\n",
              "WAGE_ABOVE_PW_HR                         9.00372\n",
              "VALIDITY_DAYS                                376\n",
              "Name: 7, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHoGs52McPfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db63ccc5-f99f-46a9-88fc-dd342c3125c1"
      },
      "source": [
        "X_reconstructed[var_columns].iloc[665]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FULL_TIME_POSITION                             Y\n",
              "PW_WAGE_LEVEL                                  I\n",
              "AGREE_TO_LC_STATEMENT                          Y\n",
              "H-1B_DEPENDENT                                 N\n",
              "WILLFUL_VIOLATOR                               N\n",
              "PUBLIC_DISCLOSURE              Disclose Business\n",
              "AGENT_REPRESENTING_EMPLOYER                    Y\n",
              "EMPLOYER_WORKSITE_YN                           Y\n",
              "OES_YN                                         N\n",
              "WAGE_ABOVE_PW_HR               9.003715529753258\n",
              "VALIDITY_DAYS                                376\n",
              "Name: 665, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3hJEc05YL6W"
      },
      "source": [
        "Decision - According to the prediction model trained on historical data, your application is expected to be [Decision - Approved/Denied].  \r\n",
        "Observations and Statistics -   \r\n",
        "1. If [employer_name] is found in RSE.categories - \r\n",
        "[employer_approval_percent]% of application by your employer [employer_name] get approved.  \r\n",
        "else ignore this observation.\r\n",
        "2. [naics_approval_percent]% of application in your NAICS code [naics_code] get approved.  \r\n",
        "3. [soc_approval_percent]% of application in your SOC code [soc_code] get approved.  \r\n",
        "if validity_days<3 years -  \r\n",
        "4. Your LCA application validity date is less than the maximum allowed period of 3 years.  \r\n",
        "\r\n",
        "Recommendations -  \r\n",
        "if Decision = Approved - Congratulations! No more recommendations.  \r\n",
        "if decision = Denied - Do grid serch on variable feature list  \r\n",
        "if model gives positive outcome for grid search -  \r\n",
        "The model predicts a positive outcome if below changes are made to the applications -  \r\n",
        "Show top 3 set of changes.  \r\n",
        "if model gives negative outcome for grid search -  \r\n",
        "Sorry, the model could not predict a positive outcome for your application based on historical data available. Contact an immigration professional for further advice.\r\n",
        "\r\n"
      ]
    }
  ]
}
